{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGqCKfg5W0tg"
      },
      "source": [
        "# Import library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAffnkRNWzWS"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSp5J4cjW24H"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AG0FTBAkUfj"
      },
      "outputs": [],
      "source": [
        "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
        "Y = np.eye(10)[y.astype(int)] # one hot encoding untuk output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ie27vAFxXhX0"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# split training and validation set\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=10000, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Value"
      ],
      "metadata": {
        "id": "DyeO91TO5y99"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JI-lP4EhLA9o"
      },
      "outputs": [],
      "source": [
        "class Value:\n",
        "  def __init__(self, data, _children=(), _op=\"\", label=\"\"):\n",
        "    self.data = data               # Data value\n",
        "    self.grad = 0                  # Grad initialization = 0\n",
        "    self._backward = lambda: None  # Local backward function\n",
        "    self._prev = set(_children)    # Previous Values\n",
        "    self._op = _op                 # Operator\n",
        "    self.label = label             # Label (variabel name, e.g., x, net, o, h)\n",
        "\n",
        "  # Data display when printed\n",
        "  def __repr__(self):\n",
        "    return f\"Value({self.data})\"\n",
        "\n",
        "  # Multiply operator\n",
        "  def __mul__(self, other):\n",
        "    # self * other\n",
        "    if isinstance(other, ValueTensor):\n",
        "      return other + self\n",
        "    elif isinstance(other, Value):\n",
        "      other = other\n",
        "    else:\n",
        "      other = Value(other)\n",
        "\n",
        "    out = Value(self.data * other.data, (self, other), \"*\")\n",
        "\n",
        "    # Local backpropagation (derivative of out w.r.t self and other)\n",
        "    def _backward():\n",
        "      other.grad += self.data * out.grad\n",
        "      self.grad += other.data * out.grad\n",
        "\n",
        "    out._backward = _backward  # add _backward function to Value out\n",
        "\n",
        "    return out\n",
        "\n",
        "  # Reverse multiply operator\n",
        "  def __rmul__(self, other):\n",
        "    # other * self\n",
        "    return self * other\n",
        "\n",
        "  # Power operator\n",
        "  def __pow__(self, other):\n",
        "    # self**other\n",
        "    if isinstance(other, (int, float)):\n",
        "      other = other\n",
        "    elif isinstance(other, Value):\n",
        "      other = float(other.data)\n",
        "    else:\n",
        "      other = float(other)\n",
        "\n",
        "    out = Value(self.data**other, (self,), f\"**{other}\")\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += other * (self.data**(other - 1)) * out.grad\n",
        "\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def __rpow__(self, other):\n",
        "    # other**self\n",
        "    if isinstance(other, (int, float)):\n",
        "      other = other\n",
        "    elif isinstance(other, Value):\n",
        "      other = float(other.data)\n",
        "    else:\n",
        "      other = float(other)\n",
        "\n",
        "    out = Value(other**self.data, (self,), f\"{other}**\")\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += out.data * np.log(other) * out.grad\n",
        "\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  # Add operator\n",
        "  def __add__(self, other):\n",
        "    # self + other\n",
        "    if isinstance(other, ValueTensor):\n",
        "      return other + self\n",
        "    elif isinstance(other, Value):\n",
        "      other = other\n",
        "    else:\n",
        "      other = Value(other)\n",
        "\n",
        "    out = Value(self.data + other.data, (self, other), \"+\")\n",
        "\n",
        "    def _backward():\n",
        "      other.grad += out.grad\n",
        "      self.grad += out.grad\n",
        "\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  # reverse add operator\n",
        "  def __radd__(self, other):\n",
        "    # other + self\n",
        "    return self + other\n",
        "\n",
        "  # negative operator\n",
        "  def __neg__(self):\n",
        "    # -self\n",
        "    return self * -1\n",
        "\n",
        "  # subtract operator\n",
        "  def __sub__(self, other):\n",
        "    # self - other\n",
        "    return self + (-other)\n",
        "\n",
        "  # reverse subtract operator\n",
        "  def __rsub__(self, other):\n",
        "    # other - self\n",
        "    return other + (-self)\n",
        "\n",
        "  # Division operator\n",
        "  def __truediv__(self, other):\n",
        "    # self / other\n",
        "    return self * other**(-1)\n",
        "\n",
        "  # reverse division operator\n",
        "  def __rtruediv__(self, other):\n",
        "    # other / self\n",
        "    return other * self**(-1)\n",
        "\n",
        "  def exp(self):\n",
        "    out = Value(np.exp(self.data), (self,), \"e**\")\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += out.data * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def log(self):\n",
        "    out = Value(np.log(self.data), (self,), \"log\")\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += 1/self.data * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def clip(self, min_val, max_val):\n",
        "    out_data = np.clip(self.data, min_val, max_val)\n",
        "    out = Value(out_data, (self,), \"clip\")\n",
        "\n",
        "    def _backward():\n",
        "        if min_val < self.data < max_val:\n",
        "            self.grad += out.grad\n",
        "        else:\n",
        "            self.grad += 0\n",
        "\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  # Global backward\n",
        "  def backward(self):\n",
        "    # Use topological order\n",
        "    topo = []\n",
        "    visited = set()\n",
        "    def build_topo(val):\n",
        "      if val not in visited:\n",
        "        visited.add(val)\n",
        "        for child in val._prev:\n",
        "          build_topo(child)\n",
        "        topo.append(val)\n",
        "\n",
        "    build_topo(self)\n",
        "\n",
        "    for val in reversed(topo):\n",
        "      val.grad = 0\n",
        "\n",
        "    # Set grad to 1 and apply the chain rule\n",
        "    self.grad = 1\n",
        "    for val in reversed(topo):\n",
        "      val._backward()\n",
        "\n",
        "  # Activation function\n",
        "  # Linear\n",
        "  def lin(self):\n",
        "    out = Value(self.data, (self,), \"Linear\")\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += 1 * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  # ReLU\n",
        "  def relu(self):\n",
        "    out = Value(max(0, self.data), (self,), \"ReLU\")\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += (0 if self.data <= 0 else 1) * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  # Sigmoid\n",
        "  def sigmoid(self):\n",
        "    out = Value(1/(1 + np.exp(-self.data)), (self,), \"Sigmoid\")\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += out.data * (1 - out.data) * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  # Hyperbolic tangent\n",
        "  def tanh(self):\n",
        "    out = Value((np.exp(self.data) - np.exp(-self.data)) \\\n",
        "                / (np.exp(self.data) + np.exp(-self.data)),\n",
        "                (self,), \"tanh\")\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += (2/(np.exp(self.data) - np.exp(-self.data)))**2 * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of value\n",
        "class ValueTensor:\n",
        "  def __init__(self, data, label=\"(h)\"):\n",
        "    if isinstance(data, ValueTensor):\n",
        "      self.shape = data.shape\n",
        "      self.dim = data.dim\n",
        "      self.data = data.data\n",
        "      self.label = data.label\n",
        "      return\n",
        "\n",
        "    if isinstance(data, (list, int, float, Value)):\n",
        "      data = np.array(data, dtype=object)\n",
        "\n",
        "    self.shape = data.shape\n",
        "    self.dim = len(self.shape)\n",
        "    if self.dim > 0:\n",
        "      ilabel = np.full(self.shape, np.arange(1, self.shape[-1]+1))\n",
        "    else:\n",
        "      ilabel = np.full(self.shape, np.arange(1, 1+1))\n",
        "    ufunc = np.frompyfunc(lambda val, i: Value(val, label=f\"{label}{i}\") if not isinstance(val, Value) else val, 2, 1)\n",
        "    self.data = ufunc(data, ilabel)\n",
        "    self.label = np.array(np.vectorize(lambda x: x.label)(self.data), dtype=object)\n",
        "\n",
        "  @property\n",
        "  def grad(self):\n",
        "    return np.vectorize(lambda x: x.grad)(self.data)\n",
        "\n",
        "  @property\n",
        "  def T(self):\n",
        "    return ValueTensor(self.data.T)\n",
        "\n",
        "  def __repr__(self):\n",
        "    if self.dim > 1:\n",
        "      return f\"ValueTensor(\\n{np.vectorize(lambda x: x.data)(self.data)})\"\n",
        "    else:\n",
        "      return f\"ValueTensor({np.vectorize(lambda x: x.data)(self.data)})\"\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item = self.data[idx]\n",
        "    if isinstance(item, np.ndarray):\n",
        "      return ValueTensor(item)\n",
        "    return item\n",
        "\n",
        "  def __setitem__(self, idx, val):\n",
        "    if isinstance(val, (int, float)):\n",
        "      self.data[idx] = Value(val)\n",
        "\n",
        "    elif isinstance(val, Value):\n",
        "      self.data[idx] = val\n",
        "\n",
        "    elif isinstance(val, (list, np.ndarray)):\n",
        "      val = np.array(val, dtype=object)\n",
        "      item = np.vectorize(lambda x: Value(x) if not isinstance(x, Value) else x)(val)\n",
        "      self.data[idx] = item\n",
        "\n",
        "    elif isinstance(val, ValueTensor):\n",
        "      self.data[idx] = val.data\n",
        "\n",
        "  def append(self, val, axis=0, label=\"b\"):\n",
        "      if isinstance(val, (int, float)):\n",
        "          val = Value(val)\n",
        "\n",
        "      elif isinstance(val, (list, np.ndarray)):\n",
        "          val = np.array(val, dtype=object)\n",
        "          val = np.vectorize(lambda x: Value(x) if not isinstance(x, Value) else x)(val)\n",
        "\n",
        "      elif isinstance(val, ValueTensor):\n",
        "          val = val.data\n",
        "\n",
        "      else:\n",
        "          raise TypeError(\"Unsupported type for append\")\n",
        "\n",
        "      new_data = np.append(self.data, val, axis=axis)\n",
        "\n",
        "      return ValueTensor(new_data, label=label)\n",
        "\n",
        "  def sum(self, axis=0, **kwargs):\n",
        "    result = np.sum(self.data, axis=axis, **kwargs)\n",
        "    return ValueTensor(result)\n",
        "\n",
        "  def mean(self, axis=0, **kwargs):\n",
        "    result = np.mean(self.data, axis=axis, **kwargs)\n",
        "    return ValueTensor(result)\n",
        "\n",
        "  def clip(self, min_val, max_val):\n",
        "    result = np.vectorize(lambda x: x.clip(min_val, max_val))(self.data)\n",
        "    return ValueTensor(result)\n",
        "\n",
        "  # Element wise addition\n",
        "  def __add__(self, other):\n",
        "    # self + other\n",
        "    if isinstance(other, (int, float, Value)):\n",
        "      if isinstance(other, Value):\n",
        "        other = other.data\n",
        "      else:\n",
        "        other = other\n",
        "      result = np.vectorize(lambda x: x + other)(self.data)\n",
        "\n",
        "    elif isinstance(other, ValueTensor):\n",
        "      if (other.dim == 0):\n",
        "        result = np.vectorize(lambda x: x + other)(self.data)\n",
        "      elif self.shape != other.shape:\n",
        "        raise ValueError(\"Shapes do not match\")\n",
        "      result = np.vectorize(lambda x, y: x + y)(self.data, other.data)\n",
        "\n",
        "    elif isinstance(other, (list, np.ndarray)):\n",
        "      other_tensor = ValueTensor(other)\n",
        "      return self + other_tensor\n",
        "\n",
        "    return ValueTensor(result)\n",
        "\n",
        "  # Element wise reverse addition\n",
        "  def __radd__(self, other):\n",
        "    # other + self\n",
        "    return self + other\n",
        "\n",
        "  # Element wise multiplication\n",
        "  def __mul__(self, other):\n",
        "    # self * other\n",
        "    if isinstance(other, (int, float, Value)):\n",
        "      if isinstance(other, Value):\n",
        "        other = other.data\n",
        "      else:\n",
        "        other = other\n",
        "      result = np.vectorize(lambda x: x * other)(self.data)\n",
        "\n",
        "    elif isinstance(other, ValueTensor):\n",
        "      if (other.dim == 0):\n",
        "        result = np.vectorize(lambda x: x * other)(self.data)\n",
        "      elif self.shape != other.shape:\n",
        "        raise ValueError(\"Shapes do not match\")\n",
        "      result = np.vectorize(lambda x, y: x * y)(self.data, other.data)\n",
        "\n",
        "    elif isinstance(other, (list, np.ndarray)):\n",
        "      other_tensor = ValueTensor(other)\n",
        "      return self * other_tensor\n",
        "\n",
        "    return ValueTensor(result)\n",
        "\n",
        "  # Element wise reverse multiplication\n",
        "  def __rmul__(self, other):\n",
        "    # other * self\n",
        "    return self * other\n",
        "\n",
        "  # Element wise power\n",
        "  def __pow__(self, other):\n",
        "    # self**other\n",
        "    if isinstance(other, (int, float, Value)):\n",
        "      if not isinstance(other, Value):\n",
        "        other = Value(other)\n",
        "      else:\n",
        "        other = other\n",
        "      result = np.vectorize(lambda x: x ** other.data)(self.data)\n",
        "\n",
        "    elif isinstance(other, ValueTensor):\n",
        "      if self.shape != other.shape:\n",
        "        raise ValueError(\"Shapes do not match\")\n",
        "      result = np.vectorize(lambda x, y: x ** y)(self.data, other.data)\n",
        "\n",
        "    elif isinstance(other, (list, np.ndarray)):\n",
        "      other_tensor = ValueTensor(other)\n",
        "      return self ** other_tensor\n",
        "\n",
        "    return ValueTensor(result)\n",
        "\n",
        "  # Element wise reverse power\n",
        "  def __rpow__(self, other):\n",
        "    # other**self\n",
        "    if isinstance(other, (int, float, Value)):\n",
        "      if not isinstance(other, Value):\n",
        "        other = Value(other)\n",
        "      else:\n",
        "        other = other\n",
        "      result = np.vectorize(lambda x: other.data ** x)(self.data)\n",
        "\n",
        "    elif isinstance(other, ValueTensor):\n",
        "      if self.shape != other.shape:\n",
        "        raise ValueError(\"Shapes do not match\")\n",
        "      result = np.vectorize(lambda x, y: x ** y)(other.data, self.data)\n",
        "\n",
        "    elif isinstance(other, (list, np.ndarray)):\n",
        "      other_tensor = ValueTensor(other)\n",
        "      return other_tensor ** self\n",
        "\n",
        "    return ValueTensor(result)\n",
        "\n",
        "  def exp(self):\n",
        "    # e**self\n",
        "    result = np.vectorize(lambda x: x.exp())(self.data)\n",
        "\n",
        "    return ValueTensor(result)\n",
        "\n",
        "  def log(self):\n",
        "    # log(self)\n",
        "    result = np.vectorize(lambda x: x.log())(self.data)\n",
        "\n",
        "    return ValueTensor(result)\n",
        "\n",
        "  def __matmul__(self, other):\n",
        "    if not isinstance(other, ValueTensor):\n",
        "        raise TypeError(f\"Cannot multiply ValueTensor with {type(other)}\")\n",
        "\n",
        "    if self.shape[-1] != other.shape[0]:\n",
        "        raise ValueError(\"Shapes do not match for matrix multiplication\")\n",
        "\n",
        "    result_data = np.empty((self.shape[0], other.shape[1]), dtype=object)\n",
        "\n",
        "    for i in range(self.shape[0]):\n",
        "        for j in range(other.shape[1]):\n",
        "            result_data[i, j] = sum(self.data[i, k] * other.data[k, j] for k in range(self.shape[1]))\n",
        "\n",
        "    return ValueTensor(result_data)\n",
        "\n",
        "  def __rmatmul__(self, other):\n",
        "    if not isinstance(other, ValueTensor):\n",
        "        raise TypeError(f\"Cannot right-multiply ValueTensor with {type(other)}\")\n",
        "\n",
        "    if other.shape[-1] != self.shape[0]:\n",
        "        raise ValueError(\"Shapes do not match for matrix multiplication\")\n",
        "\n",
        "    result_data = np.empty((other.shape[0], self.shape[1]), dtype=object)\n",
        "\n",
        "    for i in range(other.shape[0]):\n",
        "        for j in range(self.shape[1]):\n",
        "            result_data[i, j] = sum(other.data[i, k] * self.data[k, j] for k in range(other.shape[1]))\n",
        "\n",
        "    return ValueTensor(result_data)\n",
        "\n",
        "  # negative operator\n",
        "  def __neg__(self):\n",
        "    # -self\n",
        "    return self * -1\n",
        "\n",
        "  # subtract operator\n",
        "  def __sub__(self, other):\n",
        "    # self - other\n",
        "    return self + (-other)\n",
        "\n",
        "  # reverse subtract operator\n",
        "  def __rsub__(self, other):\n",
        "    # other - self\n",
        "    return other + (-self)\n",
        "\n",
        "  # Division operator\n",
        "  def __truediv__(self, other):\n",
        "    # self / other\n",
        "    return self * other**(-1)\n",
        "\n",
        "  # reverse division operator\n",
        "  def __rtruediv__(self, other):\n",
        "    # other / self\n",
        "    return other * self**(-1)\n",
        "\n",
        "  def linear(self):\n",
        "    result = np.vectorize(lambda x: x.lin())(self.data)\n",
        "    return ValueTensor(result)\n",
        "\n",
        "  def relu(self):\n",
        "    result = np.vectorize(lambda x: x.relu())(self.data)\n",
        "    return ValueTensor(result)\n",
        "\n",
        "  def sigmoid(self):\n",
        "    result = np.vectorize(lambda x: x.sigmoid())(self.data)\n",
        "    return ValueTensor(result)\n",
        "\n",
        "  def tanh(self):\n",
        "    result = np.vectorize(lambda x: x.tanh())(self.data)\n",
        "    return ValueTensor(result)\n",
        "\n",
        "  def softmax(self, axis=-1):\n",
        "    exp_data = self.exp()\n",
        "    sum_exp = np.sum(np.vectorize(lambda x: x.data)(exp_data.data), axis=axis, keepdims=True)\n",
        "\n",
        "    result = np.vectorize(lambda x, s: x / Value(s))(exp_data.data, sum_exp)\n",
        "    out = ValueTensor(result)\n",
        "\n",
        "    def _backward():\n",
        "        soft_vals = np.vectorize(lambda x: x.data)(out.data)\n",
        "        grad_output = np.vectorize(lambda x: x.grad)(out.data)\n",
        "\n",
        "        for i in range(soft_vals.shape[0]):\n",
        "            s = soft_vals[i].reshape(-1, 1)\n",
        "            jacobian = np.diagflat(s) - (s @ s.T)\n",
        "\n",
        "            grad_input = jacobian @ grad_output[i].reshape(-1, 1)\n",
        "\n",
        "            for j in range(soft_vals.shape[1]):\n",
        "                out.data[i, j].grad += grad_input[j, 0]\n",
        "\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self):\n",
        "    visited = set()\n",
        "\n",
        "    def traverse(val):\n",
        "        if val not in visited:\n",
        "            visited.add(val)\n",
        "            for child in val._prev:\n",
        "                traverse(child)\n",
        "\n",
        "    for val in np.ravel(self.data):\n",
        "        traverse(val)\n",
        "\n",
        "    for val in visited: # set gradien 0\n",
        "        val.grad = 0\n",
        "\n",
        "    for val in np.ravel(self.data):\n",
        "        val.grad = 1\n",
        "\n",
        "    topo = []\n",
        "    visited_topo = set()\n",
        "\n",
        "    def build_topo(val):\n",
        "        if val not in visited_topo:\n",
        "            visited_topo.add(val)\n",
        "            for child in val._prev:\n",
        "                build_topo(child)\n",
        "            topo.append(val)\n",
        "\n",
        "    for val in np.ravel(self.data):\n",
        "        build_topo(val)\n",
        "\n",
        "    for val in reversed(topo):\n",
        "        val._backward()\n",
        "\n",
        "class criterion:\n",
        "  # Loss\n",
        "  # MSE\n",
        "  def mse(y_true, y_pred):\n",
        "    y_true = ValueTensor(y_true)\n",
        "    y_pred = ValueTensor(y_pred)\n",
        "\n",
        "    mean_ = ((y_true-y_pred)**2).mean(axis=-1)\n",
        "    mean_ = ValueTensor(np.expand_dims(mean_.data, axis=0))\n",
        "    return mean_.mean(axis=-1)\n",
        "\n",
        "  # BCE\n",
        "  def binary_cross_entropy(y_true, y_pred):\n",
        "    y_true = ValueTensor(y_true)\n",
        "    y_pred = ValueTensor(y_pred.clip(1e-10, 1 - 1e-10))\n",
        "\n",
        "    t1 = y_pred.log()\n",
        "    t2 = y_true * t1\n",
        "    t3 = (1 - y_true)\n",
        "    t4 = (1 - y_pred).log()\n",
        "    t5 = t3 * t4\n",
        "    t6 = t2 + t5\n",
        "    t7 = t6.mean(axis=-1)\n",
        "    t7 = ValueTensor(np.expand_dims(t7.data, axis=0))\n",
        "    t8 = t7.mean(axis=-1)\n",
        "    return -t8\n",
        "\n",
        "  # CCE\n",
        "  def categorical_cross_entropy(y_true, y_pred):\n",
        "    y_true = ValueTensor(y_true)\n",
        "    y_pred = ValueTensor(y_pred.clip(1e-10, 1 - 1e-10))\n",
        "\n",
        "    t1 = y_pred.log()\n",
        "    t2 = y_true *  t1\n",
        "    t3 = t2.sum(axis=-1)\n",
        "    t3 = ValueTensor(np.expand_dims(t3.data, axis=0))\n",
        "    t4 = t3.mean(axis=-1)\n",
        "    return -t4\n",
        "\n",
        "  # derivatives\n",
        "  # output hasil yang belum dikali turunan fungsi aktivasi\n",
        "  def mse_errors(y_true, y_pred):\n",
        "    return -2 * (y_true - y_pred) / y_pred.shape[0]\n",
        "\n",
        "  def bce_errors(y_true, y_pred):\n",
        "    return -1 * (y_pred - y_true) / (y_pred * (1 - y_pred) * y_pred.shape[0])\n",
        "\n",
        "  def cce_errors(y_true, y_pred):\n",
        "    return -1 * y_true / (y_pred * y_pred.shape[0])"
      ],
      "metadata": {
        "id": "JxKmvX8aEP6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Layer"
      ],
      "metadata": {
        "id": "woVqnfmA568q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e05mKhULjhzK"
      },
      "outputs": [],
      "source": [
        "class initialization:\n",
        "  # kelas untuk inisialisasi bobot tiap neuron layer\n",
        "  # beberapa cara inisialisasi: zero, uniform/normal distribution, xavier/he(bonus)\n",
        "  # size: tuple (jumlah neuron input, jumlah neuron output)\n",
        "  def zero(size):\n",
        "    return np.zeros(size)\n",
        "\n",
        "  def uniform(size, lower_bound=-1, upper_bound=1, seed=None, method=\"random\"):\n",
        "    if seed is not None:\n",
        "      np.random.seed(seed)\n",
        "\n",
        "    low, high = lower_bound, upper_bound\n",
        "    if (method == \"xavier\"):\n",
        "      x = np.sqrt(6 / (size[0] + size[1]))\n",
        "      low, high = -x, x\n",
        "    elif (method == \"he\"):\n",
        "      x = np.sqrt(6 / size[0])\n",
        "      low, high = -x, x\n",
        "\n",
        "    return np.random.uniform(low=low, high=high, size=size)\n",
        "\n",
        "  def normal(size, mean=0, std=1, seed=None, method=\"random\"):\n",
        "    if seed is not None:\n",
        "      np.random.seed(seed)\n",
        "\n",
        "    loc, scale = mean, std\n",
        "    if (method == \"xavier\"):\n",
        "      loc, scale = 0, np.sqrt(2 / (size[0] + size[1]))\n",
        "    elif (method == \"he\"):\n",
        "      loc, scale = 0, np.sqrt(2 / size[0])\n",
        "\n",
        "    return np.random.normal(loc=loc, scale=scale, size=size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "  def __init__(self, input_size, output_size, activation_function=\"linear\", weight_init=\"normal\", weight_low_or_mean=None, weight_high_or_std=1, weight_seed=None, weight_type=\"random\"):\n",
        "    self.input_size = input_size # jumlah neuron di dalam layer ini\n",
        "    self.output_size = output_size # jumlah neuron di layer selanjutnya. untuk weight\n",
        "    self.activation_function = activation_function # string??? idk. activation function yg digunakan\n",
        "\n",
        "    # weight_init (string): zero, uniform, atau normal\n",
        "    # weight_low_or_mean: untuk lower bound kalau pakai uniform atau mean kalau pakai normal\n",
        "    # weight_high_or_std: untuk upper bound kalau pakai uniform atau std kalau pakai normal\n",
        "    # weight_seed: seed untuk inisialisasi weight. untuk reproducibility\n",
        "    # weight_type (string): random, xavier (bonus), atau he (bonus)\n",
        "    # inisialisasi weight semua neuron dan bias. size = (input_size + 1, output_size)\n",
        "    weights_array = None\n",
        "    if (weight_init == \"uniform\"):\n",
        "      if (weight_low_or_mean == None): weights_array = initialization.uniform((input_size + 1, output_size), -1, weight_high_or_std, weight_seed, weight_type)\n",
        "      else: weights_array = initialization.uniform((input_size + 1, output_size), weight_low_or_mean, weight_high_or_std, weight_seed, weight_type)\n",
        "    elif (weight_init == \"normal\"):\n",
        "      if (weight_low_or_mean == None): weights_array = initialization.normal((input_size + 1, output_size), 0, weight_high_or_std, weight_seed, weight_type)\n",
        "      else: weights_array = initialization.normal((input_size + 1, output_size), weight_low_or_mean, weight_high_or_std, weight_seed, weight_type)\n",
        "    else: # weight_init == \"zero\"\n",
        "      weights_array = initialization.zero((input_size + 1, output_size))\n",
        "    self.weights = ValueTensor(weights_array)\n",
        "\n",
        "    self.neuron_values = None # berisikan semua nilai neuron dan bias dalam satu layer, dalam satu batch??? yang neuronnya sudah dikasih fungsi aktivasi\n",
        "    self.next_raw = None # untuk simpan data nilai layer selanjutnya yang belum diberi fungsi aktivasi\n",
        "    self.next_activated = None # untuk simpan data nilai layer selanjutnya yang sudah diberi fungsi aktivasi\n",
        "    self.next_error = None # untuk simpan gradien untuk backpropagation dan update weight\n",
        "\n",
        "  def forward(self, inputs): # untuk forward propagation\n",
        "    if not isinstance(inputs, ValueTensor): inputs = ValueTensor(inputs)\n",
        "\n",
        "    self.neuron_values = ValueTensor(np.hstack((inputs.data, np.ones((inputs.shape[0], 1))))) # sekalian isiin bias\n",
        "    self.next_raw = self.neuron_values @ self.weights\n",
        "\n",
        "    # activation function\n",
        "    if (self.activation_function == \"relu\"): self.next_activated = self.next_raw.relu()\n",
        "    elif (self.activation_function == \"sigmoid\"): self.next_activated = self.next_raw.sigmoid()\n",
        "    elif (self.activation_function == \"tanh\"): self.next_activated = self.next_raw.tanh()\n",
        "    elif (self.activation_function == \"softmax\"): self.next_activated = self.next_raw.softmax()\n",
        "    else: self.next_activated = self.next_raw.linear() # activation function == \"linear\"\n",
        "\n",
        "    return self.next_activated\n",
        "\n",
        "  def backward_and_update_weights(self, next_gradients, learning_rate, is_last): # untuk back propagation dan sekaligus update weights\n",
        "    # next_gradients itu error layer selanjutnya lagi yg sudah dikaliin dengan weights layer selanjutnya\n",
        "    # is_last: true kalau bukan yang terakhir\n",
        "    if not isinstance(next_gradients, ValueTensor): next_gradients = ValueTensor(next_gradients)\n",
        "\n",
        "    self.next_activated.backward() # untuk self.next_raw.grad\n",
        "\n",
        "    self.next_error = ValueTensor(self.next_raw.grad) * next_gradients\n",
        "\n",
        "    if not is_last:\n",
        "      weight_T_no_bias = ValueTensor(np.array([row[:-1] for row in self.weights.data.T], dtype=object))\n",
        "\n",
        "    # update weights\n",
        "    self.weights -= learning_rate * (ValueTensor(self.neuron_values.data.T) @ self.next_error)\n",
        "\n",
        "    if not is_last: return (self.next_error @ weight_T_no_bias)\n",
        "    else: return"
      ],
      "metadata": {
        "id": "3VOyzJoX5F_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OutputLayer:\n",
        "  def __init__(self, output_size, loss_function=\"mse\"):\n",
        "    self.predicted = None # ValueTensor matriks (batch_size, output_size) prediksi\n",
        "    self.target = None # target output\n",
        "    self.loss_function = loss_function # mse, bce, cce\n",
        "    self.loss = None # nilai loss\n",
        "    self.loss_derivatives = None # nilai hasil turunan loss yang belum dikali turunan nilai output layer dan sudah dibagi batch size. matriks\n",
        "\n",
        "  def setPredictions(self, predicted, target):\n",
        "    if not isinstance(predicted, ValueTensor): self.predicted = ValueTensor(predicted)\n",
        "    else: self.predicted = predicted\n",
        "    if not isinstance(target, ValueTensor): self.target = ValueTensor(target)\n",
        "    else: self.target = target\n",
        "\n",
        "  def calculateLoss(self):\n",
        "    # asumsi sudah ada self.predicted dan self.target\n",
        "    if (self.loss_function == \"bce\"): self.loss = criterion.binary_cross_entropy(self.target, self.predicted)\n",
        "    elif (self.loss_function == \"cce\"): self.loss = criterion.categorical_cross_entropy(self.target, self.predicted)\n",
        "    else: self.loss = criterion.mse(self.target, self.predicted) # self.loss_function == \"mse\"\n",
        "\n",
        "  def lossDerivatives(self):\n",
        "    # asumsi sudah ada self.predicted dan self.targets\n",
        "    if (self.loss_function == \"bce\"): self.loss_derivatives = criterion.bce_errors(self.target, self.predicted)\n",
        "    elif (self.loss_function == \"cce\"): self.loss_derivatives = criterion.cce_errors(self.target, self.predicted)\n",
        "    else: self.loss_derivatives = criterion.mse_errors(self.target, self.predicted) # self.loss_function == \"mse\""
      ],
      "metadata": {
        "id": "4fI5hN5Ne95w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "zReEM5GK5-TD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AvoJP_xYymW"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm  # Progress bar\n",
        "import pickle\n",
        "\n",
        "class FFNN:\n",
        "  def __init__(self, input_size, hidden_size_array, output_size, activation_function, loss_function, weight_init):\n",
        "    self.input_size = input_size # 784\n",
        "    self.hidden_size_array = np.array(hidden_size_array).astype(int) # array jumlah neuron tiap hidden layer\n",
        "    self.output_size = output_size # 10\n",
        "    self.num_neurons = np.insert(hidden_size_array, 0, input_size)\n",
        "    self.num_neurons = np.append(self.num_neurons, output_size).astype(int) # array jumlah neuron termasuk input dan output layer\n",
        "    self.activation_function = activation_function # array fungsi aktivasi setiap layer (termasuk output)\n",
        "    self.loss_function = loss_function # hanya untuk output layer. MSE, Binary cross entropy, atau Categorical cross entropy\n",
        "    self.weight_init = weight_init # array tuple (weight_init, weight_low_or_mean, weight_high_or_std, weight_seed, weight_type) inisialisasi bobot tiap layer (termasuk input)\n",
        "\n",
        "    # asumsi model punya minimal satu hidden layer\n",
        "    self.input_and_hidden_layers = [Layer(self.num_neurons[i], self.num_neurons[i+1], activation_function[i], weight_init[i][0], weight_init[i][1], weight_init[i][2], weight_init[i][3], weight_init[i][4]) for i in range (len(hidden_size_array) + 1)]\n",
        "    self.output_layer = OutputLayer(output_size, loss_function)\n",
        "\n",
        "  def forward_propagation(self, data, target):\n",
        "    # forward propagation satu kali dalam satu batch\n",
        "    values = data\n",
        "    for i in range (len(self.input_and_hidden_layers)):\n",
        "      values = self.input_and_hidden_layers[i].forward(values)\n",
        "    self.output_layer.setPredictions(values, target)\n",
        "    self.output_layer.calculateLoss()\n",
        "    return self.output_layer.loss\n",
        "\n",
        "  def back_propagation(self, learning_rate):\n",
        "    # backward propagation satu kali dalam satu batch\n",
        "    # asumsi sudah melakukan forward_propagation sebelum ini\n",
        "    self.output_layer.lossDerivatives()\n",
        "    values = self.output_layer.loss_derivatives\n",
        "    for i in range (len(self.input_and_hidden_layers) - 1, 0, -1):\n",
        "      values = self.input_and_hidden_layers[i].backward_and_update_weights(values, learning_rate, False)\n",
        "    self.input_and_hidden_layers[0].backward_and_update_weights(values, learning_rate, True)\n",
        "    return\n",
        "\n",
        "  def train_model(self, batch_size, learning_rate, num_epochs, x_train, y_train, x_val, y_val, verbose=0):\n",
        "    # verbose == 0: tidak menampilkan apa-apa\n",
        "    # verbose == 1: menampilkan progress bar, kondisi training_loss dan validation_loss\n",
        "    X_batches_train = np.array_split(x_train, np.ceil(len(x_train) / batch_size))\n",
        "    Y_batches_train = np.array_split(y_train, np.ceil(len(y_train) / batch_size))\n",
        "    X_batches_val = np.array_split(x_val, np.ceil(len(x_val) / batch_size))\n",
        "    Y_batches_val = np.array_split(y_val, np.ceil(len(y_val) / batch_size))\n",
        "    num_of_batches_train = len(X_batches_train)\n",
        "    num_of_batches_val = len(X_batches_val)\n",
        "    training_loss_array = []\n",
        "    val_loss_array = []\n",
        "    batches_loss_array = []\n",
        "    for i in range (num_epochs):\n",
        "      progress = range(num_of_batches_train + num_of_batches_val)\n",
        "      if (verbose == 1): # show progress bar\n",
        "        progress = tqdm(progress, desc=f\"Epoch {i+1}/{num_epochs}\", unit=\"batch\")\n",
        "\n",
        "      batches_loss_array.clear()\n",
        "      for j in range (num_of_batches_train):\n",
        "        batches_loss_array.append(self.forward_propagation(X_batches_train[j], Y_batches_train[j]))\n",
        "        if (verbose == 1):\n",
        "          progress.set_postfix({\"Batch Loss\": batches_loss_array[j]})\n",
        "          progress.update(1)\n",
        "        self.back_propagation(learning_rate)\n",
        "      training_loss_array.append(batches_loss_array.mean())\n",
        "      batches_loss_array.clear()\n",
        "      for j in range (num_of_batches_val):\n",
        "        batches_loss_array.append(self.forward_propagation(X_batches_val[j], Y_batches_val[j]))\n",
        "      val_loss_array.append(batches_loss_array.mean())\n",
        "\n",
        "      if (verbose == 1):\n",
        "        (print(f\"Epoch {i+1}: Train Loss = {training_loss_array[i]}, Val Loss = {val_loss_array[i]}\"))\n",
        "    return training_loss_array, val_loss_array\n",
        "\n",
        "  # def weight_distribution\n",
        "\n",
        "  # def gradient_distribution\n",
        "\n",
        "  def save_model(self, filename):\n",
        "    with open(filename, \"wb\") as f:\n",
        "      pickle.dump(self, f)\n",
        "    print(f\"Model saved to {filename}\")\n",
        "\n",
        "  @staticmethod\n",
        "  def load_model(filename):\n",
        "    with open(filename, \"rb\") as f:\n",
        "      model = pickle.load(f)\n",
        "    print(f\"Model loaded from {filename}\")\n",
        "    return model"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}